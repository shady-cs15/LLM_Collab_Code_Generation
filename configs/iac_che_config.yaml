# model
model:
  name: "Qwen/Qwen2.5-Coder-3B"
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "bfloat16"

# dataset
dataset:
  name: "OpenMLRL/CoopHumanEval"
  type: "coophumaneval"
  train_split: "test[16:]"
  eval_split: "test[:16]"

# output
output:
  base_dir: "output"
  verbose: false
  save_final_model: false
  save_path: "output/maac_final"

# external
external:
  mode: "level_feedback"
  sandbox_slice: 1

# iac
iac:
  num_turns: 2
  num_train_epochs: 80
  per_device_train_batch_size: 1
  actor_learning_rate: 5.0e-6
  critic_learning_rate: 5.0e-6
  value_loss_coef: 0.6
  rollout_buffer_size: 4
  max_new_tokens: 256
  temperature: 0.6
  top_p: 0.6
  top_k: null
  num_agents: 2
  critic_model: null
  reward_shift: -4
  discount: 0.9
  early_termination_threshold: -0.2
  eval_interval: 20
  eval_num_samples: 4

  logging_steps: 5
# wandb
wandb:
  project: "comlrl"
  entity: "OpenMLRL"
  name: "iac_coophumaneval"
  dir: "output"
  tags: ["iac", "coophumaneval", "multi-agent", "turns_2"]
