# Configuration for HumanEval training with MAGRPO
# This file defines all parameters for training experiments

# Model configuration
model:
  name: "Qwen/Qwen2.5-Coder-3B"  # Options: "Qwen/Qwen2.5-Coder-3B", "bigcode/starcoder2-3b", etc.
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "auto"

# Dataset configuration
dataset:
  name: "openai/openai_humaneval"
  type: "humaneval"  # Used to select formatters and reward function
  train_split: "test[33:133]"
  eval_split: "test[:32]"

# Output configuration
output:
  base_dir: "../../../projects/bepg/sliu30/output"
  save_final_model: true

# MAGRPO training configuration
magrpo:
  num_train_epochs: 10
  per_device_train_batch_size: 1
  learning_rate: 1.0e-5
  logging_steps: 50
  save_steps: 200
  num_generations: 4
  max_new_tokens: 256
  beta: 0.02
  num_agents: 2
  expert_model: "deepseek-coder"  # Expert model for feedback (optional for single-turn)
  # Temperature and top_p will be set based on model_config if not specified
  # temperature: 0.8  # Optional override
  # top_p: 0.95      # Optional override

# Wandb configuration
wandb:
  project: "mlrl"
  entity: "nu-llpr"
  name: "magrpo_humaneval"  # Will be appended with model name in script
  dir: "../../../projects/bepg/sliu30"
  tags: ["magrpo", "humaneval", "multi-agent"]