# Configuration for Multi-Turn CoopHumanEval training with GRPO (single-agent)
# Based on mt_magrpo_che_config.yaml parameters but adapted for single-agent

# Model configuration
model:
  name: "Qwen/Qwen2.5-Coder-3B"
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "bfloat16"

# Dataset configuration
dataset:
  name: "LovelyBuggies/CoopHumaneval"
  type: "coophumaneval"  # Used to select formatters and reward function
  train_split: "test[:50]"
  eval_split: "test[50:66]"

# Output configuration
output:
  base_dir: "../../../projects/bevi/sliu30/output_mt"
  save_final_model: true

# GRPO training configuration (multi-turn enabled via num_turns)
grpo:
  num_turns: 2
  num_train_epochs: 10  # Reduced from 20 for multi-turn
  per_device_train_batch_size: 1
  learning_rate: 2.0e-5
  logging_steps: 50
  save_steps: 200
  num_generations: 4
  max_new_tokens: 256
  temperature: 0.8
  top_p: 0.95
  # Multi-turn specific parameters
  turn_gradient_weights: [1.2, 0.8]
  early_termination_weight: 2.0
  early_termination_threshold: 2.1
  external_mode: "expert_edits"    # Options: expert_edits (default), level_passed, level_feedback, passed, plain
  expert_model: "deepseek-coder"   # Used by expert_edits mode only

# Wandb configuration
wandb:
  project: "mlrl"
  entity: "nu-llpr"
  name: "mt_grpo_coophumaneval"  # Will be appended with model name in script
  dir: "../../../projects/bevi/sliu30"
  tags: ["mt_grpo", "coophumaneval", "single-agent", "multi-turn"]
