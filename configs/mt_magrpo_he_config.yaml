# model
model:
  name: "Qwen/Qwen2.5-Coder-3B"
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "bfloat16"

# dataset
dataset:
  name: "openai/openai_humaneval"
  type: "humaneval"
  train_split: "test[33:163]"
  eval_split: "test[:32]"

# output
output:
  base_dir: "../../../work/hdd/bepg/sliu30/output_mt_magrpo"
  save_final_model: false

# external
external:
  mode: "level_feedback"
  sandbox_slice: 1
  original_prompt: true
  previous_response: true

# magrpo
magrpo:
  num_turns: 2
  num_train_epochs: 6
  per_device_train_batch_size: 1
  learning_rate: 2.0e-5
  logging_steps: 50
  save_steps: 200
  num_generations: 4
  max_new_tokens: 256
  num_agents: 2
  handoff: random
  turn_gradient_weights: [1.2, 0.8]
  early_termination_weight: 2.0
  early_termination_threshold: 4.0

# wandb
wandb:
  project: "mlrl"
  entity: "nu-llpr"
  name: "mt_magrpo_humaneval"
  dir: "../../../work/hdd/bepg/sliu30/output_mt_magrpo"
  tags: ["mt_magrpo", "humaneval", "multi-agent", "multi-turn"]
